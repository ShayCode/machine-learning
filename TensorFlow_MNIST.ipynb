{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "The problem chosen here is referred to as the \"Hello World\" for machine learning. The dataset is called MNIST and refers to handwritten digit recognition. This dataset was developed by Yann LeCun (Director of AI Research, Facebook). The dataset provides 28x28 images of handwritten digits (1 per image) and the goal is to write an algorithm that detects which digit is written. Since there are only 10 digits, this is a classification problem with 10 classes. Built a network with 2 hidden layers between inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Anaconda\\envs\\python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Anaconda\\envs\\python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Anaconda\\envs\\python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Anaconda\\envs\\python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Anaconda\\envs\\python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-7793bda27fc1>:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Anaconda\\envs\\python3.6\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Anaconda\\envs\\python3.6\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda\\envs\\python3.6\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda\\envs\\python3.6\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Anaconda\\envs\\python3.6\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-5fbc87cda20c>:37: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch 1. Mean loss: 0.428. Validation loss: 0.200. Validation accuracy: 94.70%\n",
      "Epoch 2. Mean loss: 0.187. Validation loss: 0.157. Validation accuracy: 95.76%\n",
      "Epoch 3. Mean loss: 0.142. Validation loss: 0.132. Validation accuracy: 96.06%\n",
      "Epoch 4. Mean loss: 0.115. Validation loss: 0.111. Validation accuracy: 96.66%\n",
      "Epoch 5. Mean loss: 0.098. Validation loss: 0.105. Validation accuracy: 97.10%\n",
      "Epoch 6. Mean loss: 0.085. Validation loss: 0.103. Validation accuracy: 96.76%\n",
      "Epoch 7. Mean loss: 0.074. Validation loss: 0.094. Validation accuracy: 97.08%\n",
      "Epoch 8. Mean loss: 0.064. Validation loss: 0.093. Validation accuracy: 97.26%\n",
      "Epoch 9. Mean loss: 0.058. Validation loss: 0.090. Validation accuracy: 97.38%\n",
      "Epoch 10. Mean loss: 0.051. Validation loss: 0.094. Validation accuracy: 97.02%\n",
      "End of training.\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 50\n",
    "\n",
    "# Reset any variables left in memory from previous runs.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# Declare placeholders where the data will be fed into.\n",
    "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "# Weights and biases for the first linear combination between the inputs and the first hidden layer.\n",
    "weights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
    "biases_1 = tf.get_variable(\"biases_1\", [hidden_layer_size])\n",
    "\n",
    "# Operation between the inputs and the first hidden layer.\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
    "\n",
    "# Weights and biases for the second linear combination.\n",
    "# This is between the first and second hidden layers.\n",
    "weights_2 = tf.get_variable(\"weights_2\", [hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.get_variable(\"biases_2\", [hidden_layer_size])\n",
    "\n",
    "# Operation between the first and the second hidden layers. Used Relu Activation\n",
    "outputs_2 = tf.nn.relu(tf.matmul(outputs_1, weights_2) + biases_2)\n",
    "\n",
    "# Weights and biases for the final linear combination.\n",
    "# That's between the second hidden layer and the output layer.\n",
    "weights_3 = tf.get_variable(\"weights_3\", [hidden_layer_size, output_size])\n",
    "biases_3 = tf.get_variable(\"biases_3\", [output_size])\n",
    "\n",
    "# Operation between the second hidden layer and the final output.\n",
    "outputs = tf.matmul(outputs_2, weights_3) + biases_3\n",
    "\n",
    "# Calculate the loss function for every output/target pair.\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=targets)\n",
    "\n",
    "# Get the average loss\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "# Defining the optimization step. Using adaptive optimizers such as Adam in TensorFlow\n",
    "optimize = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)\n",
    "\n",
    "# Get a 0 or 1 for every input in the batch indicating whether it output the correct answer out of the 10.\n",
    "out_equals_target = tf.equal(tf.argmax(outputs, 1), tf.argmax(targets, 1))\n",
    "\n",
    "# Get the average accuracy of the outputs.\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
    "\n",
    "# Declare the session variable.\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Initialize the variables. Default initializer is Xavier.\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "# Batching\n",
    "batch_size = 100\n",
    "\n",
    "# Calculate the number of batches per epoch for the training set.\n",
    "batches_number = mnist.train._num_examples // batch_size\n",
    "\n",
    "# Basic early stopping. Set a miximum number of epochs.\n",
    "max_epochs = 15\n",
    "\n",
    "# Keep track of the validation loss of the previous epoch.\n",
    "# If the validation loss starts increasing,  trigger early stopping.\n",
    "prev_validation_loss = 9999999.\n",
    "\n",
    "\n",
    "# Create a loop for the epochs. Epoch_counter automatically starts from 0.\n",
    "for epoch_counter in range(max_epochs):\n",
    "    \n",
    "    # Keep track of the sum of batch losses in the epoch.\n",
    "    curr_epoch_loss = 0.\n",
    "    \n",
    "    # Iterate over the batches in this epoch.\n",
    "    for batch_counter in range(batches_number):\n",
    "        \n",
    "        # Input batch and target batch are assigned values from the train dataset, given a batch size\n",
    "        input_batch, target_batch = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # Run the optimization step and get the mean loss for this batch.\n",
    "        # Feed it with the inputs and the targets we just got from the train dataset\n",
    "        _, batch_loss = sess.run([optimize, mean_loss], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "        # Increment the sum of batch losses.\n",
    "        curr_epoch_loss += batch_loss\n",
    "    \n",
    "   \n",
    "    # the average batch losses over the whole epoch\n",
    "    curr_epoch_loss /= batches_number\n",
    "    \n",
    "    # Get the input batch and the target batch from the validation dataset\n",
    "    input_batch, target_batch = mnist.validation.next_batch(mnist.validation._num_examples)\n",
    "    \n",
    "   \n",
    "    validation_loss, validation_accuracy = sess.run([mean_loss, accuracy], \n",
    "        feed_dict={inputs: input_batch, targets: target_batch})\n",
    "    \n",
    "    # Print statistics for the current epoch\n",
    "    print('Epoch '+str(epoch_counter+1)+\n",
    "          '. Mean loss: '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "          '. Validation loss: '+'{0:.3f}'.format(validation_loss)+\n",
    "          '. Validation accuracy: '+'{0:.2f}'.format(validation_accuracy * 100.)+'%')\n",
    "    \n",
    "    # Trigger early stopping if validation loss starts increasing.\n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "        \n",
    "    # Store this epoch's validation loss to be used as previous validation loss in the next iteration.\n",
    "    prev_validation_loss = validation_loss\n",
    "\n",
    "print('End of training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.95%\n"
     ]
    }
   ],
   "source": [
    "input_batch, target_batch = mnist.test.next_batch(mnist.test._num_examples)\n",
    "test_accuracy = sess.run([accuracy], \n",
    "    feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "# print (test_accuracy)\n",
    "test_accuracy_percent = test_accuracy[0] * 100.\n",
    "\n",
    "# Print the test accuracy formatted in percentages\n",
    "print('Test accuracy: '+'{0:.2f}'.format(test_accuracy_percent)+'%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3.6] *",
   "language": "python",
   "name": "conda-env-python3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
